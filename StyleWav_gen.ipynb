{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleCLIP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f2dc86cd6a694687ba5e565280e57116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3263b5b2b588465cad49372cab85f614",
              "IPY_MODEL_47ff67d8f10745568834ae8b4ec7e43b",
              "IPY_MODEL_84c91c515c0744a6958a0aa279cf8c9a"
            ],
            "layout": "IPY_MODEL_ed4ddfc89a0c42d4926fafb0ae6daea1"
          }
        },
        "3263b5b2b588465cad49372cab85f614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_298e57c749a34394b10f9fc6e70cdc3f",
            "placeholder": "​",
            "style": "IPY_MODEL_0ebd57b3a02d4f5e9fed2a754c44518f",
            "value": "100%"
          }
        },
        "47ff67d8f10745568834ae8b4ec7e43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ac630a3d84c491fab8f7fb536ef9851",
            "max": 48968059,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebb0d7fb887a4a879fdc4a774f5257e2",
            "value": 48968059
          }
        },
        "84c91c515c0744a6958a0aa279cf8c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_830c8db14b48404fb575ef38b7cc5430",
            "placeholder": "​",
            "style": "IPY_MODEL_c32864b082bb4c1f958a17fb5ee25c4f",
            "value": " 46.7M/46.7M [00:00&lt;00:00, 107MB/s]"
          }
        },
        "ed4ddfc89a0c42d4926fafb0ae6daea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "298e57c749a34394b10f9fc6e70cdc3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ebd57b3a02d4f5e9fed2a754c44518f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ac630a3d84c491fab8f7fb536ef9851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebb0d7fb887a4a879fdc4a774f5257e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "830c8db14b48404fb575ef38b7cc5430": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c32864b082bb4c1f958a17fb5ee25c4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install wav2clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrABs-N7eyiZ",
        "outputId": "a9c29edc-cf46-49c1-8447-9f60f30a9226"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ep087kv5\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-ep087kv5\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.63.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.11.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Collecting wav2clip\n",
            "  Downloading wav2clip-0.1.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (from wav2clip) (0.10.0+cu111)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from wav2clip) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from wav2clip) (1.21.5)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (from wav2clip) (0.8.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (0.51.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (21.3)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (1.1.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (2.1.9)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (0.10.3.post1)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa->wav2clip) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa->wav2clip) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa->wav2clip) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa->wav2clip) (3.0.7)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->wav2clip) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa->wav2clip) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->wav2clip) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->wav2clip) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->wav2clip) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->wav2clip) (1.24.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa->wav2clip) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa->wav2clip) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa->wav2clip) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa->wav2clip) (2.21)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->wav2clip) (3.10.0.2)\n",
            "Installing collected packages: wav2clip\n",
            "Successfully installed wav2clip-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o karras2019stylegan-ffhq-1024x1024.for_g_all.pt -C - 'https://github.com/lernapparat/lernapparat/releases/download/v2019-02-01/karras2019stylegan-ffhq-1024x1024.for_g_all.pt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJvIXQotfwLL",
        "outputId": "c974cb57-9baa-48d2-84bc-597dea25965b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   685  100   685    0     0   3186      0 --:--:-- --:--:-- --:--:--  3186\n",
            "100  100M  100  100M    0     0  55.7M      0  0:00:01  0:00:01 --:--:-- 85.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rdfDIMueeli5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import clip\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "mPBC8IAHezph"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import wav2clip"
      ],
      "metadata": {
        "id": "4ZO2wU_Dp6pV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLinear(nn.Module):\n",
        "    \"\"\"Linear layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
        "    def __init__(self, input_size, output_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True):\n",
        "        super().__init__()\n",
        "        he_std = gain * input_size**(-0.5) # He init\n",
        "        # Equalized learning rate and custom learning rate multiplier.\n",
        "        if use_wscale:\n",
        "            init_std = 1.0 / lrmul\n",
        "            self.w_mul = he_std * lrmul\n",
        "        else:\n",
        "            init_std = he_std / lrmul\n",
        "            self.w_mul = lrmul\n",
        "        self.weight = torch.nn.Parameter(torch.randn(output_size, input_size) * init_std)\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(output_size))\n",
        "            self.b_mul = lrmul\n",
        "        else:\n",
        "            self.bias = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            bias = bias * self.b_mul\n",
        "        return F.linear(x, self.weight * self.w_mul, bias)\n",
        "\n",
        "\n",
        "\n",
        "class MyConv2d(nn.Module):\n",
        "    \"\"\"Conv layer with equalized learning rate and custom learning rate multiplier.\"\"\"\n",
        "    def __init__(self, input_channels, output_channels, kernel_size, gain=2**(0.5), use_wscale=False, lrmul=1, bias=True,\n",
        "                intermediate=None, upscale=False):\n",
        "        super().__init__()\n",
        "        if upscale:\n",
        "            self.upscale = Upscale2d()\n",
        "        else:\n",
        "            self.upscale = None\n",
        "        he_std = gain * (input_channels * kernel_size ** 2) ** (-0.5) # He init\n",
        "        self.kernel_size = kernel_size\n",
        "        if use_wscale:\n",
        "            init_std = 1.0 / lrmul\n",
        "            self.w_mul = he_std * lrmul\n",
        "        else:\n",
        "            init_std = he_std / lrmul\n",
        "            self.w_mul = lrmul\n",
        "        self.weight = torch.nn.Parameter(torch.randn(output_channels, input_channels, kernel_size, kernel_size) * init_std)\n",
        "        if bias:\n",
        "            self.bias = torch.nn.Parameter(torch.zeros(output_channels))\n",
        "            self.b_mul = lrmul\n",
        "        else:\n",
        "            self.bias = None\n",
        "        self.intermediate = intermediate\n",
        "\n",
        "    def forward(self, x):\n",
        "        bias = self.bias\n",
        "        if bias is not None:\n",
        "            bias = bias * self.b_mul\n",
        "        \n",
        "        have_convolution = False\n",
        "        if self.upscale is not None and min(x.shape[2:]) * 2 >= 128:\n",
        "            # this is the fused upscale + conv from StyleGAN, sadly this seems incompatible with the non-fused way\n",
        "            # this really needs to be cleaned up and go into the conv...\n",
        "            w = self.weight * self.w_mul\n",
        "            w = w.permute(1, 0, 2, 3)\n",
        "            # probably applying a conv on w would be more efficient. also this quadruples the weight (average)?!\n",
        "            w = F.pad(w, (1,1,1,1))\n",
        "            w = w[:, :, 1:, 1:]+ w[:, :, :-1, 1:] + w[:, :, 1:, :-1] + w[:, :, :-1, :-1]\n",
        "            x = F.conv_transpose2d(x, w, stride=2, padding=(w.size(-1)-1)//2)\n",
        "            have_convolution = True\n",
        "        elif self.upscale is not None:\n",
        "            x = self.upscale(x)\n",
        "    \n",
        "        if not have_convolution and self.intermediate is None:\n",
        "            return F.conv2d(x, self.weight * self.w_mul, bias, padding=self.kernel_size//2)\n",
        "        elif not have_convolution:\n",
        "            x = F.conv2d(x, self.weight * self.w_mul, None, padding=self.kernel_size//2)\n",
        "        \n",
        "        if self.intermediate is not None:\n",
        "            x = self.intermediate(x)\n",
        "        if bias is not None:\n",
        "            x = x + bias.view(1, -1, 1, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class NoiseLayer(nn.Module):\n",
        "    \"\"\"adds noise. noise is per pixel (constant over channels) with per-channel weight\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(channels))\n",
        "        self.noise = None\n",
        "    \n",
        "    def forward(self, x, noise=None):\n",
        "        if noise is None and self.noise is None:\n",
        "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
        "        elif noise is None:\n",
        "            # here is a little trick: if you get all the noiselayers and set each\n",
        "            # modules .noise attribute, you can have pre-defined noise.\n",
        "            # Very useful for analysis\n",
        "            noise = self.noise\n",
        "        x = x + self.weight.view(1, -1, 1, 1) * noise\n",
        "        return x  \n",
        "\n",
        "\n",
        "class StyleMod(nn.Module):\n",
        "    def __init__(self, latent_size, channels, use_wscale):\n",
        "        super(StyleMod, self).__init__()\n",
        "        self.lin = MyLinear(latent_size,\n",
        "                            channels * 2,\n",
        "                            gain=1.0, use_wscale=use_wscale)\n",
        "        \n",
        "    def forward(self, x, latent):\n",
        "        style = self.lin(latent) # style => [batch_size, n_channels*2]\n",
        "        shape = [-1, 2, x.size(1)] + (x.dim() - 2) * [1]\n",
        "        style = style.view(shape)  # [batch_size, 2, n_channels, ...]\n",
        "        x = x * (style[:, 0] + 1.) + style[:, 1]\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelNormLayer(nn.Module):\n",
        "    \"\"\" This layer ensures that the input vector have std = 1:\n",
        "        - std = 1/N * sqrt(x-mean(x))\n",
        "        - In this case x comes from a normal centred in 0 so mean(x) = 0\n",
        "        - Dividing the value of std(x) to x makes the normal of x become 1:\n",
        "            + If the std == 1 it remains the same\n",
        "            + If the std > 1 it decrease the values of x which in consequence reduce the std\n",
        "            + If the std < 1 it increase the values of x which in consequence increase the std\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "    def forward(self, x):\n",
        "        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + self.epsilon)\n",
        "\n",
        "\n",
        "class BlurLayer(nn.Module):\n",
        "    def __init__(self, kernel=[1, 2, 1], normalize=True, flip=False, stride=1):\n",
        "        super(BlurLayer, self).__init__()\n",
        "        kernel=[1, 2, 1]\n",
        "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
        "        kernel = kernel[:, None] * kernel[None, :]\n",
        "        kernel = kernel[None, None]\n",
        "        if normalize:\n",
        "            kernel = kernel / kernel.sum()\n",
        "        if flip:\n",
        "            kernel = kernel[:, :, ::-1, ::-1]\n",
        "        self.register_buffer('kernel', kernel)\n",
        "        self.stride = stride\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # expand kernel channels\n",
        "        kernel = self.kernel.expand(x.size(1), -1, -1, -1)\n",
        "        x = F.conv2d(\n",
        "            x,\n",
        "            kernel,\n",
        "            stride=self.stride,\n",
        "            padding=int((self.kernel.size(2)-1)/2),\n",
        "            groups=x.size(1)\n",
        "        )\n",
        "        return x\n",
        "\n",
        "\n",
        "def upscale2d(x, factor=2, gain=1):\n",
        "    assert x.dim() == 4\n",
        "    if gain != 1:\n",
        "        x = x * gain\n",
        "    if factor != 1:\n",
        "        shape = x.shape\n",
        "        x = x.view(shape[0], shape[1], shape[2], 1, shape[3], 1).expand(-1, -1, -1, factor, -1, factor)\n",
        "        x = x.contiguous().view(shape[0], shape[1], factor * shape[2], factor * shape[3])\n",
        "    return x\n",
        "\n",
        "\n",
        "class Upscale2d(nn.Module):\n",
        "    def __init__(self, factor=2, gain=1):\n",
        "        super().__init__()\n",
        "        assert isinstance(factor, int) and factor >= 1\n",
        "        self.gain = gain\n",
        "        self.factor = factor\n",
        "    def forward(self, x):\n",
        "        return upscale2d(x, factor=self.factor, gain=self.gain)\n",
        "\n",
        "\n",
        "class G_mapping(nn.Sequential):\n",
        "    def __init__(self, nonlinearity='lrelu', use_wscale=True):\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "        layers = [\n",
        "            ('pixel_norm', PixelNormLayer()),\n",
        "            ('dense0', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense0_act', act),\n",
        "            ('dense1', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense1_act', act),\n",
        "            ('dense2', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense2_act', act),\n",
        "            ('dense3', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense3_act', act),\n",
        "            ('dense4', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense4_act', act),\n",
        "            ('dense5', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense5_act', act),\n",
        "            ('dense6', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense6_act', act),\n",
        "            ('dense7', MyLinear(512, 512, gain=gain, lrmul=0.01, use_wscale=use_wscale)),\n",
        "            ('dense7_act', act)\n",
        "        ]\n",
        "        super().__init__(OrderedDict(layers))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = super().forward(x)\n",
        "        # Broadcast\n",
        "        x = x.expand(-1, 18, -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Truncation(nn.Module):\n",
        "    def __init__(self, avg_latent, max_layer=8, threshold=0.7):\n",
        "        super().__init__()\n",
        "        self.max_layer = max_layer\n",
        "        self.threshold = threshold\n",
        "        self.register_buffer('avg_latent', avg_latent) # parameter of the module which is not trainable and is not passed to the optimizer when calling parameters() function\n",
        "    def forward(self, x):\n",
        "        assert x.dim() == 3\n",
        "        interp = torch.lerp(self.avg_latent, x, self.threshold)\n",
        "        do_trunc = (torch.arange(x.size(1)) < self.max_layer).view(1, -1, 1)\n",
        "        return torch.where(do_trunc, interp, x)\n",
        "\n",
        "\n",
        "class LayerEpilogue(nn.Module):\n",
        "    \"\"\"Things to do at the end of each layer.\"\"\"\n",
        "    def __init__(self, channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        if use_noise:\n",
        "            layers.append(('noise', NoiseLayer(channels)))\n",
        "        layers.append(('activation', activation_layer))\n",
        "        if use_pixel_norm:\n",
        "            layers.append(('pixel_norm', PixelNormLayer()))\n",
        "        if use_instance_norm:\n",
        "            layers.append(('instance_norm', nn.InstanceNorm2d(channels)))\n",
        "        self.top_epi = nn.Sequential(OrderedDict(layers))\n",
        "        if use_styles:\n",
        "            self.style_mod = StyleMod(dlatent_size, channels, use_wscale=use_wscale)\n",
        "        else:\n",
        "            self.style_mod = None\n",
        "    def forward(self, x, dlatents_in_slice=None):\n",
        "        x = self.top_epi(x)\n",
        "        if self.style_mod is not None:\n",
        "            x = self.style_mod(x, dlatents_in_slice)\n",
        "        else:\n",
        "            assert dlatents_in_slice is None\n",
        "        return x\n",
        "\n",
        "\n",
        "class InputBlock(nn.Module):\n",
        "    def __init__(self, nf, dlatent_size, const_input_layer, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
        "        super().__init__()\n",
        "        self.const_input_layer = const_input_layer\n",
        "        self.nf = nf\n",
        "        if self.const_input_layer:\n",
        "            # called 'const' in tf\n",
        "            self.const = nn.Parameter(torch.ones(1, nf, 4, 4))\n",
        "            self.bias = nn.Parameter(torch.ones(nf))\n",
        "        else:\n",
        "            self.dense = MyLinear(dlatent_size, nf*16, gain=gain/4, use_wscale=use_wscale) # tweak gain to match the official implementation of Progressing GAN\n",
        "        self.epi1 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "        self.conv = MyConv2d(nf, nf, 3, gain=gain, use_wscale=use_wscale)\n",
        "        self.epi2 = LayerEpilogue(nf, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "        \n",
        "    def forward(self, dlatents_in_range):\n",
        "        batch_size = dlatents_in_range.size(0)\n",
        "        if self.const_input_layer:\n",
        "            x = self.const.expand(batch_size, -1, -1, -1)\n",
        "            x = x + self.bias.view(1, -1, 1, 1)\n",
        "        else:\n",
        "            x = self.dense(dlatents_in_range[:, 0]).view(batch_size, self.nf, 4, 4)\n",
        "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
        "        x = self.conv(x)\n",
        "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GSynthesisBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer):\n",
        "        # 2**res x 2**res # res = 3..resolution_log2\n",
        "        super().__init__()\n",
        "        if blur_filter:\n",
        "            blur = BlurLayer(blur_filter)\n",
        "        else:\n",
        "            blur = None\n",
        "        self.conv0_up = MyConv2d(in_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale,\n",
        "                                 intermediate=blur, upscale=True)\n",
        "        self.epi1 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "        self.conv1 = MyConv2d(out_channels, out_channels, kernel_size=3, gain=gain, use_wscale=use_wscale)\n",
        "        self.epi2 = LayerEpilogue(out_channels, dlatent_size, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, activation_layer)\n",
        "            \n",
        "    def forward(self, x, dlatents_in_range):\n",
        "        x = self.conv0_up(x)\n",
        "        x = self.epi1(x, dlatents_in_range[:, 0])\n",
        "        x = self.conv1(x)\n",
        "        x = self.epi2(x, dlatents_in_range[:, 1])\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class G_synthesis(nn.Module):\n",
        "    def __init__(self,\n",
        "        dlatent_size        = 512,          # Disentangled latent (W) dimensionality.\n",
        "        num_channels        = 3,            # Number of output color channels.\n",
        "        resolution          = 1024,         # Output resolution.\n",
        "        fmap_base           = 8192,         # Overall multiplier for the number of feature maps.\n",
        "        fmap_decay          = 1.0,          # log2 feature map reduction when doubling the resolution.\n",
        "        fmap_max            = 512,          # Maximum number of feature maps in any layer.\n",
        "        use_styles          = True,         # Enable style inputs?\n",
        "        const_input_layer   = True,         # First layer is a learned constant?\n",
        "        use_noise           = True,         # Enable noise inputs?\n",
        "        randomize_noise     = True,         # True = randomize noise inputs every time (non-deterministic), False = read noise inputs from variables.\n",
        "        nonlinearity        = 'lrelu',      # Activation function: 'relu', 'lrelu'\n",
        "        use_wscale          = True,         # Enable equalized learning rate?\n",
        "        use_pixel_norm      = False,        # Enable pixelwise feature vector normalization?\n",
        "        use_instance_norm   = True,         # Enable instance normalization?\n",
        "        dtype               = torch.float32,  # Data type to use for activations and outputs.\n",
        "        blur_filter         = [1,2,1],      # Low-pass filter to apply when resampling activations. None = no filtering.\n",
        "        ):\n",
        "        \n",
        "        super().__init__()\n",
        "        def nf(stage):\n",
        "            return min(int(fmap_base / (2.0 ** (stage * fmap_decay))), fmap_max)\n",
        "        self.dlatent_size = dlatent_size\n",
        "        resolution_log2 = int(np.log2(resolution))\n",
        "        assert resolution == 2**resolution_log2 and resolution >= 4\n",
        "\n",
        "        act, gain = {'relu': (torch.relu, np.sqrt(2)),\n",
        "                     'lrelu': (nn.LeakyReLU(negative_slope=0.2), np.sqrt(2))}[nonlinearity]\n",
        "        num_layers = resolution_log2 * 2 - 2\n",
        "        num_styles = num_layers if use_styles else 1\n",
        "        torgbs = []\n",
        "        blocks = []\n",
        "        for res in range(2, resolution_log2 + 1):\n",
        "            channels = nf(res-1)\n",
        "            name = '{s}x{s}'.format(s=2**res)\n",
        "            if res == 2:\n",
        "                blocks.append((name,\n",
        "                               InputBlock(channels, dlatent_size, const_input_layer, gain, use_wscale,\n",
        "                                      use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
        "                \n",
        "            else:\n",
        "                blocks.append((name,\n",
        "                               GSynthesisBlock(last_channels, channels, blur_filter, dlatent_size, gain, use_wscale, use_noise, use_pixel_norm, use_instance_norm, use_styles, act)))\n",
        "            last_channels = channels\n",
        "        self.torgb = MyConv2d(channels, num_channels, 1, gain=1, use_wscale=use_wscale)\n",
        "        self.blocks = nn.ModuleDict(OrderedDict(blocks))\n",
        "        \n",
        "    def forward(self, dlatents_in):\n",
        "        # Input: Disentangled latents (W) [minibatch, num_layers, dlatent_size].\n",
        "        # lod_in = tf.cast(tf.get_variable('lod', initializer=np.float32(0), trainable=False), dtype)\n",
        "        batch_size = dlatents_in.size(0)       \n",
        "        for i, m in enumerate(self.blocks.values()):\n",
        "            if i == 0:\n",
        "                x = m(dlatents_in[:, 2*i:2*i+2])\n",
        "            else:\n",
        "                x = m(x, dlatents_in[:, 2*i:2*i+2])\n",
        "        rgb = self.torgb(x)\n",
        "        return rgb\n",
        "\n",
        "\n",
        "avg_latent = torch.zeros(1, 18, 512)\n",
        "\n",
        "g_all = nn.Sequential(OrderedDict([\n",
        "    ('g_mapping', G_mapping()),\n",
        "    # ('truncation', Truncation(avg_latent)),\n",
        "    ('g_synthesis', G_synthesis())    \n",
        "]))\n",
        "g_all.load_state_dict(torch.load('karras2019stylegan-ffhq-1024x1024.for_g_all.pt'))\n",
        "\n",
        "g_synthesis = g_all.g_synthesis\n",
        "g_mapping = g_all.g_mapping"
      ],
      "metadata": {
        "id": "-t4UU9PZfHB9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def saveFeatureMaps(model, layer=1):\n",
        "    root = './feature_maps'\n",
        "    if not os.path.exists(root):\n",
        "        os.mkdir(root)\n",
        "    for idx, m in enumerate(model.linear):\n",
        "        print(m)\n",
        "        if isinstance(m, nn.Linear):\n",
        "            print('Linear ', idx)\n",
        "            weight = m.weight.view(18,-1,512)\n",
        "            print(weight.size())\n",
        "            for s in range(weight.size(1)):\n",
        "                w1 = weight[:,s,:]\n",
        "                for w in w1:\n",
        "                    w = sorted(w.detach().cpu().numpy(), reverse=True)\n",
        "                    print(w[0:10])\n",
        "                print('\\n')\n",
        "\n",
        "            # for w in range(weight.size(1)):\n",
        "            #     print(weight[:,w,:], end='\\n')\n",
        "            #     break\n",
        "            break\n",
        "\n",
        "\n",
        "            # img = np.moveaxis(weight.cpu().detach().numpy(), 0, -1)\n",
        "            # img_resized = np.moveaxis(img_resized[0].cpu().detach().numpy().squeeze(), 0, -1)\n",
        "\n",
        "            # img_array = img*255\n",
        "            # Image.fromarray(img_array.astype(np.uint8)).resize((400, 400)).save(current_dir + '/img' + str(0) + '.png')\n",
        "\n",
        "            # img_array = img_resized*255\n",
        "            # Image.fromarray(img_array.astype(np.uint8)).resize((400, 400)).save(current_dir + '/img_resized' + str(0) + '.png')\n",
        "\n",
        "            # f_map = np.moveaxis(feature_maps_ref[0].cpu().detach().numpy().squeeze(), 0, -1)\n",
        "\n",
        "            # if not os.path.exists(current_dir):\n",
        "            #     os.mkdir(current_dir)\n",
        "\n",
        "            # for f in range(f_map.shape[-1]):\n",
        "            #     img_array = f_map[:,:,f]\n",
        "            #     img_array = ((img_array - np.min(f_map))/np.max(f_map))*255\n",
        "            #     f_map_img = Image.fromarray(img_array.astype(np.uint8)).convert('L').resize((400, 400))\n",
        "                \n",
        "            #     img_array = img_resized*255\n",
        "            #     img_resized_img = Image.fromarray(img_array.astype(np.uint8)).resize((400, 400))\n",
        "                \n",
        "            #     img = np.expand_dims(np.asarray(f_map_img), 2).repeat(3,2)/255 * np.asarray(img_resized_img)\n",
        "            #     Image.fromarray(img.astype(np.uint8)).save(current_dir + '/feature_map_' + str(0) + '_' + str(f) + '.png')\n",
        "\n",
        "\n",
        "class GetFeatureMaps(nn.Module):\n",
        "    \"\"\" \n",
        "        GetFeatureMaps: model that returns the flattened feature maps in a certain layer for a certain model\n",
        "        \n",
        "         \"\"\"\n",
        "    def __init__(self, model = None, layer = 45):\n",
        "         \n",
        "        super(GetFeatureMaps, self).__init__()\n",
        "\n",
        "        # The Sequential element is the first dictionary of the model and it's where we can find the inception modules.\n",
        "        self.feature_map = nn.Sequential(*model[0:layer+1])\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        return self.feature_map(imgs).view(imgs.size(0), -1)\n",
        "\n",
        "\n",
        "def transform_img(img, img_size):\n",
        "    transforms = [\n",
        "        T.Resize((img_size, img_size)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # mean and std of the imageNet dataset\n",
        "    ]\n",
        "    transform = T.Compose(transforms)\n",
        "    \n",
        "    return transform(img).unsqueeze(0)\n",
        "\n",
        "\n",
        "def compute_loss(f_maps, f_maps_ref, lambdas):\n",
        "    assert len(f_maps) == len(f_maps_ref)\n",
        "\n",
        "    num_f_maps = len(f_maps)\n",
        "    loss = 0\n",
        "    for i in range(num_f_maps):\n",
        "        loss += (nn.functional.l1_loss(f_maps[i], f_maps_ref[i])/num_f_maps)*lambdas[i]\n",
        "        # print(\"LOSSS \" + str(i) + \" -->\",  nn.functional.l1_loss(f_map_norm, f_map_ref_norm))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "pXFTsGfqfXw0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = 'generations'\n",
        "batch_size = 1\n",
        "prompt = \"The image of an young asian lady\"\n",
        "lr = 1e-2\n",
        "img_save_freq = 100\n",
        "ref_img_path = None\n",
        "max_iter = 500"
      ],
      "metadata": {
        "id": "Cc7sLKA4hOst"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_prompt = \"girl_talking.wav\""
      ],
      "metadata": {
        "id": "9tmjoqaipeA4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wav2clip_model = wav2clip.get_model()\n",
        "audio, sr = librosa.load(audio_prompt, sr=16000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "f2dc86cd6a694687ba5e565280e57116",
            "3263b5b2b588465cad49372cab85f614",
            "47ff67d8f10745568834ae8b4ec7e43b",
            "84c91c515c0744a6958a0aa279cf8c9a",
            "ed4ddfc89a0c42d4926fafb0ae6daea1",
            "298e57c749a34394b10f9fc6e70cdc3f",
            "0ebd57b3a02d4f5e9fed2a754c44518f",
            "6ac630a3d84c491fab8f7fb536ef9851",
            "ebb0d7fb887a4a879fdc4a774f5257e2",
            "830c8db14b48404fb575ef38b7cc5430",
            "c32864b082bb4c1f958a17fb5ee25c4f"
          ]
        },
        "id": "9ejR_s8vpqkG",
        "outputId": "37784169-5741-4843-9ba7-f8d5e13e3d87"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/descriptinc/lyrebird-wav2clip/releases/download/v0.1.0-alpha/Wav2CLIP.pt\" to /root/.cache/torch/hub/checkpoints/Wav2CLIP.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/46.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2dc86cd6a694687ba5e565280e57116"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = output_path\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"USING \", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRWVhrhxhRny",
        "outputId": "a75dc2ec-4ce6-4b84-c551-af59fd574a27"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "USING  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True).to(device)\n",
        "vgg_layers = vgg16.features\n",
        "\n",
        "vgg_layer_name_mapping = {\n",
        "    '1': \"relu1_1\",\n",
        "    '3': \"relu1_2\",\n",
        "    '6': \"relu2_1\",\n",
        "    '8': \"relu2_2\",\n",
        "    # '15': \"relu3_3\",\n",
        "    # '22': \"relu4_3\"\n",
        "}"
      ],
      "metadata": {
        "id": "2xP8WHiaiIbo"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_synthesis.eval()\n",
        "g_synthesis.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbyXf5XIje_-",
        "outputId": "ccd2fb84-eefa-4a95-c7c0-44bf9702b8d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "G_synthesis(\n",
              "  (torgb): MyConv2d()\n",
              "  (blocks): ModuleDict(\n",
              "    (4x4): InputBlock(\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8x8): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (16x16): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (32x32): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (64x64): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (128x128): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (256x256): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (512x512): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (1024x1024): GSynthesisBlock(\n",
              "      (conv0_up): MyConv2d(\n",
              "        (upscale): Upscale2d()\n",
              "        (intermediate): BlurLayer()\n",
              "      )\n",
              "      (epi1): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "      (conv1): MyConv2d()\n",
              "      (epi2): LayerEpilogue(\n",
              "        (top_epi): Sequential(\n",
              "          (noise): NoiseLayer()\n",
              "          (activation): LeakyReLU(negative_slope=0.2)\n",
              "          (instance_norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
              "        )\n",
              "        (style_mod): StyleMod(\n",
              "          (lin): MyLinear()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_shape = (batch_size, 1, 512)\n",
        "\n",
        "normal_generator = torch.distributions.normal.Normal(\n",
        "    torch.tensor([0.0]),\n",
        "    torch.tensor([1.]),\n",
        ")\n",
        "\n",
        "# init_latents = normal_generator.sample(latent_shape).squeeze(-1).to(device)\n",
        "latents_init = torch.zeros(latent_shape).squeeze(-1).to(device)\n",
        "latents = torch.nn.Parameter(latents_init, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    params=[latents],\n",
        "    lr=lr,\n",
        "    betas=(0.9, 0.999),\n",
        ")\n"
      ],
      "metadata": {
        "id": "hAzwJ5pRjhHm"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncation(x, threshold=0.7, max_layer=8):\n",
        "    avg_latent = torch.zeros(batch_size, x.size(1), 512).to(device)\n",
        "    interp = torch.lerp(avg_latent, x, threshold)\n",
        "    do_trunc = (torch.arange(x.size(1)) < max_layer).view(1, -1, 1).to(device)\n",
        "    return torch.where(do_trunc, interp, x)\n",
        "\n",
        "def tensor_to_pil_img(img):\n",
        "    img = (img.clamp(-1, 1) + 1) / 2.0\n",
        "    img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
        "    img = Image.fromarray(img.astype('uint8'))\n",
        "    return img\n",
        "\n",
        "\n",
        "clip_transform = torchvision.transforms.Compose([\n",
        "    # clip_preprocess.transforms[2],\n",
        "    clip_preprocess.transforms[4],\n",
        "])\n",
        "\n",
        "if ref_img_path is None:\n",
        "    ref_img = None\n",
        "else:\n",
        "    ref_img = clip_preprocess(Image.open(ref_img_path)).unsqueeze(0).to(device)\n",
        "\n",
        "clip_normalize = torchvision.transforms.Normalize(\n",
        "    mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "    std=(0.26862954, 0.26130258, 0.27577711),\n",
        ")\n",
        "\n",
        "def compute_clip_loss(img, text):\n",
        "    # img = clip_transform(img)\n",
        "    img = torch.nn.functional.upsample_bilinear(img, (224, 224))\n",
        "\n",
        "    text_features = torch.from_numpy(wav2clip.embed_audio(audio, wav2clip_model)).to(device).to(torch.float16)\n",
        "    image_features = clip_model.encode_image(img)\n",
        "\n",
        "    logit_scale = clip_model.logit_scale.exp()\n",
        "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
        "    logits_per_text = logits_per_image.t()\n",
        "    \n",
        "    return 1/logits_per_image * 100\n",
        "\n",
        "def compute_perceptual_loss(gen_img, ref_img):\n",
        "    gen_img = torch.nn.functional.upsample_bilinear(img, (224, 224))\n",
        "    loss = 0\n",
        "    len_vgg_layer_mappings = int(max(vgg_layer_name_mapping.keys()))\n",
        "\n",
        "    ref_feats = ref_img\n",
        "    gen_feats = gen_img\n",
        "\n",
        "    for idx, (name, module) in enumerate(vgg_layers._modules.items()):\n",
        "        ref_feats = module(ref_feats)\n",
        "        gen_feats = module(gen_feats)\n",
        "        if name in vgg_layer_name_mapping.keys():\n",
        "            loss += torch.nn.functional.mse_loss(ref_feats, gen_feats)\n",
        "        \n",
        "        if idx >= len_vgg_layer_mappings:\n",
        "            break\n",
        "    \n",
        "    return loss/len_vgg_layer_mappings"
      ],
      "metadata": {
        "id": "-8Svs3pBjkDQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter = 0\n",
        "while True:\n",
        "    dlatents = latents.repeat(1,18,1)\n",
        "    img = g_synthesis(dlatents)\n",
        "    \n",
        "    # NOTE: clip normalization did not seem to have much effect\n",
        "    # img = clip_normalize(img)\n",
        "\n",
        "    loss = compute_clip_loss(img, prompt)\n",
        "\n",
        "    # NOTE: uncomment to use perceptual loos. Still WIP. You will need to define\n",
        "    # the `ref_img_path` to use it. The image referenced will be the one \n",
        "    # used to condition the generation.\n",
        "    # perceptual_loss = compute_perceptual_loss(img, ref_img)\n",
        "    # loss = loss + perceptual_loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter % img_save_freq == 0:\n",
        "        img = tensor_to_pil_img(img)\n",
        "        img.save(os.path.join(output_dir, f'{counter}.png'))\n",
        "\n",
        "        print(f'Step {counter}')\n",
        "        print(f'Loss {loss.data.cpu().numpy()[0][0]}')\n",
        "\n",
        "    counter += 1\n",
        "    if(counter > max_iter): break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVc6IkQZjqke",
        "outputId": "a706b0c3-0839-47e0-813e-da34b64e9711"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3847: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0\n",
            "Loss 0.047210693359375\n",
            "Step 100\n",
            "Loss 0.042572021484375\n",
            "Step 200\n",
            "Loss 0.036407470703125\n",
            "Step 300\n",
            "Loss 0.036407470703125\n",
            "Step 400\n",
            "Loss 0.033538818359375\n",
            "Step 500\n",
            "Loss 0.03253173828125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "szHH3EOntO5l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}